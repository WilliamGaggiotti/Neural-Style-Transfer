{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordered-engagement",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import scipy.io\n",
    "import scipy.misc\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "from tensorflow.keras import backend as K\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pprint\n",
    "import imageio\n",
    "import cv2\n",
    "from tensorflow.keras.applications import vgg19\n",
    "from tensorflow import keras\n",
    "from tensorflow import keras\n",
    "from IPython.display import Image, display\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "religious-slovenia",
   "metadata": {},
   "source": [
    "## Name of the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepting-brake",
   "metadata": {},
   "outputs": [],
   "source": [
    "STYLE_IMAGE_NAME = \"el_grito.jpg\"\n",
    "CONTENT_IMAGE_NAME = \"amigos2_el_grito.jpeg\"\n",
    "GENERATED_NAME = \"amigos2_el_grito\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "checked-intention",
   "metadata": {},
   "source": [
    "## Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "domestic-september",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLOR_CHANNELS = 3\n",
    "NOISE_RATIO = 0.6\n",
    "#coefficient to scale the different errors\n",
    "TOTAL_VARIATION_COEFF = 1e-6\n",
    "STYLE_COEFF = 1e-6\n",
    "CONTENT_COEFF = 2.5e-8\n",
    "#Path of images\n",
    "STYLE_PATH = \"style_images/van_gogh/\"\n",
    "CONTENT_PATH = \"amigos2_el_grito/\"\n",
    "GENERATED_PATH = \"generated/\"\n",
    "STYLE_IMAGE_PATH = STYLE_PATH+STYLE_IMAGE_NAME\n",
    "CONTENT_IMAGE_PATH = STYLE_PATH+CONTENT_PATH+CONTENT_IMAGE_NAME\n",
    "GENERATED_IMAGE_PATH = STYLE_PATH+CONTENT_PATH+GENERATED_PATH+GENERATED_NAME\n",
    "BASE_WIDTH ,BASE_HEIGTH = keras.preprocessing.image.load_img(CONTENT_IMAGE_PATH).size\n",
    "IMG_NROWS = 400\n",
    "IMG_NCOLS = int(BASE_WIDTH * IMG_NROWS / BASE_HEIGTH)\n",
    "#IMG_NCOLS = 1024\n",
    "#IMG_NROWS = int(BASE_HEIGTH* IMG_NCOLS / BASE_WIDTH)\n",
    "print(BASE_WIDTH ,BASE_HEIGTH)\n",
    "STYLE_IMAGE_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjacent-twenty",
   "metadata": {},
   "source": [
    "## Style Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vital-rover",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(STYLE_IMAGE_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facial-delaware",
   "metadata": {},
   "source": [
    "## Content Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proprietary-slope",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(CONTENT_IMAGE_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ignored-riverside",
   "metadata": {},
   "source": [
    "## Load model pretrained vgg19\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sound-hampshire",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = vgg19.VGG19(weights=\"imagenet\", include_top=False)\n",
    "\n",
    "# Get the symbolic outputs of each \"key\" layer (we gave them unique names).\n",
    "outputs_dict = dict([(layer.name, layer.output) for layer in model.layers])\n",
    "\n",
    "# Set up a model that returns the activation values for every layer in\n",
    "# VGG19 (as a dict).\n",
    "feature_extractor = keras.Model(inputs=model.inputs, outputs=outputs_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expanded-luxembourg",
   "metadata": {},
   "source": [
    "## Functions for preprocessings images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlling-portrait",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path):\n",
    "    # Util function to open, resize and format pictures into appropriate tensors\n",
    "    img = keras.preprocessing.image.load_img(\n",
    "        image_path, target_size=(IMG_NROWS, IMG_NCOLS)\n",
    "    )\n",
    "    img = keras.preprocessing.image.img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = vgg19.preprocess_input(img)\n",
    "    return tf.convert_to_tensor(img)\n",
    "\n",
    "\n",
    "def deprocess_image(x):\n",
    "    # Util function to convert a tensor into a valid image\n",
    "    x = x.reshape((IMG_NROWS, IMG_NCOLS, 3))\n",
    "    # Remove zero-center by mean pixel\n",
    "    x[:, :, 0] += 103.939\n",
    "    x[:, :, 1] += 116.779\n",
    "    x[:, :, 2] += 123.68\n",
    "    # 'BGR'->'RGB'\n",
    "    x = x[:, :, ::-1]\n",
    "    x = np.clip(x, 0, 255).astype(\"uint8\")\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "illegal-confusion",
   "metadata": {},
   "source": [
    "## Neural Style Transfer (NST)\n",
    "\n",
    "we will need to define 3 things:\n",
    "\n",
    "- Build the content cost function $J_{content}(C,G)$\n",
    "- Build the style cost function $J_{style}(S,G)$\n",
    "- Build the total variation cost fuction $J_{total_variation}(G)$\n",
    "- Put it together to get $J(G) = \\alpha J_{content}(C,G) + \\beta J_{style}(S,G)$ +  $J_{total variation}(G)$ \n",
    "\n",
    "### Content cost\n",
    "\n",
    "$$J_{content}(C,G) =  \\frac{1}{4 \\times n_H \\times n_W \\times n_C}\\sum _{ \\text{all entries}} (a^{(C)} - a^{(G)})^2\\tag{1} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tamil-belief",
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_cost(a_C, a_G):\n",
    "    \n",
    "    return tf.reduce_sum(tf.square(a_C - a_G))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indoor-agent",
   "metadata": {},
   "source": [
    "## Style cost\n",
    "we need to define:\n",
    "\n",
    "- Style matrix or Gram matrix\n",
    "- style_cost\n",
    "\n",
    "### Style matrix \n",
    "* The style matrix is also called a \"Gram matrix.\" \n",
    "* In linear algebra, the Gram matrix G of a set of vectors $(v_{1},\\dots ,v_{n})$ is the matrix of dot products, whose entries are ${\\displaystyle G_{ij} = v_{i}^T v_{j} = np.dot(v_{i}, v_{j})  }$. \n",
    "* In other words, $G_{ij}$ compares how similar $v_i$ is to $v_j$: If they are highly similar, you would expect them to have a large dot product, and thus for $G_{ij}$ to be large. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "above-video",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(A):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    A -- matrix of shape (n_H, n_W, n_C)\n",
    "    \n",
    "    Returns:\n",
    "    GA -- Gram matrix of A, of shape (n_C, n_C)\n",
    "    \"\"\"\n",
    "    \n",
    "    A_T = tf.transpose(A, (2, 0, 1))\n",
    "    \n",
    "    A_T_reshaped = tf.reshape(A_T, (tf.shape(A_T)[0], -1))\n",
    "    \n",
    "    GA = tf.matmul(A_T_reshaped, tf.transpose(A_T_reshaped))\n",
    "    \n",
    "    return GA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controversial-pickup",
   "metadata": {},
   "source": [
    "## Style_cost\n",
    "\n",
    "$$J_{style}^{[l]}(S,G) = \\frac{1}{4 \\times {n_C}^2 \\times (n_H \\times n_W)^2} \\sum _{i=1}^{n_C}\\sum_{j=1}^{n_C}(G^{(S)}_{(gram)i,j} - G^{(G)}_{(gram)i,j})^2\\tag{2} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "warming-penetration",
   "metadata": {},
   "outputs": [],
   "source": [
    "def style_cost(a_S, a_G):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    a_S -- tensor of dimension (n_H, n_W, n_C), hidden layer activations representing style of the image S \n",
    "    a_G -- tensor of dimension (n_H, n_W, n_C), hidden layer activations representing style of the image G\n",
    "    \n",
    "    Returns: \n",
    "    J_style_layer -- tensor representing a scalar value\n",
    "    \"\"\"\n",
    "    G_a_S = gram_matrix(a_S)\n",
    "    G_a_G = gram_matrix(a_G)\n",
    "    \n",
    "    return tf.reduce_sum(tf.square(G_a_S - G_a_G)) / (4.0 * (COLOR_CHANNELS ** 2) * ((IMG_NROWS * IMG_NCOLS) ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appropriate-mediterranean",
   "metadata": {},
   "source": [
    "## Total variation cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sexual-pasta",
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_variation_cost(a_G):\n",
    "    a = tf.square(\n",
    "        a_G[:, : IMG_NROWS - 1, : IMG_NCOLS - 1, :] - a_G[:, 1:, : IMG_NCOLS - 1, :]\n",
    "    )\n",
    "    b = tf.square(\n",
    "        a_G[:, : IMG_NROWS - 1, : IMG_NCOLS - 1, :] - a_G[:, : IMG_NROWS - 1, 1:, :]\n",
    "    )\n",
    "    \n",
    "    total_variation_loss = tf.reduce_sum(tf.pow(a + b, 1.25))\n",
    "   \n",
    "    return total_variation_loss * TOTAL_VARIATION_COEFF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arranged-cleaners",
   "metadata": {},
   "source": [
    "## Layers of model VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saved-shopping",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of layers to use for the style loss.\n",
    "style_layer_names = [\n",
    "    \"block1_conv1\",\n",
    "    \"block2_conv1\",\n",
    "    \"block3_conv1\",\n",
    "    \"block4_conv1\",\n",
    "    \"block5_conv1\",\n",
    "]\n",
    "# The layer to use for the content loss.\n",
    "content_layer_name = \"block5_conv2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "therapeutic-neutral",
   "metadata": {},
   "source": [
    "## Total Cost\n",
    "$$J(G) = \\alpha J_{content}(C,G) + \\beta J_{style}(S,G)$ + J_{total variation}(G)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hydraulic-catalog",
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_cost(a_S, a_G, a_C):\n",
    "    \n",
    "    input_tensor = tf.concat(\n",
    "        [a_C, a_S, a_G], axis=0\n",
    "    )\n",
    "    features = feature_extractor(input_tensor)\n",
    "\n",
    "    # Initialize the loss\n",
    "    loss = tf.zeros(shape=())\n",
    "\n",
    "    # Add content loss\n",
    "    layer_features = features[content_layer_name]\n",
    "    a_C_features = layer_features[0, :, :, :]\n",
    "    a_G_features = layer_features[2, :, :, :]\n",
    "    loss = loss + CONTENT_COEFF * content_cost(\n",
    "        a_C_features, a_G_features\n",
    "    )\n",
    "    # Add style loss\n",
    "    for layer_name in style_layer_names:\n",
    "        layer_features = features[layer_name]\n",
    "        a_S_features = layer_features[1, :, :, :]\n",
    "        a_G_features = layer_features[2, :, :, :]\n",
    "        sl = style_cost(a_S_features, a_G_features)\n",
    "        loss += (STYLE_COEFF / len(style_layer_names)) * sl\n",
    "\n",
    "    # Add total variation loss\n",
    "    loss += TOTAL_VARIATION_COEFF * total_variation_cost(a_G)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "least-liberia",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def compute_loss_and_grads(style_image, generated_image, content_image):\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = total_cost(style_image, generated_image, content_image)\n",
    "        \n",
    "    grads = tape.gradient(loss, generated_image)\n",
    "    \n",
    "    return loss, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shared-animal",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(\n",
    "    keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=100.0, decay_steps=100, decay_rate=0.96\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "'''optimizer = keras.optimizers.Adam(\n",
    "    keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=100.0, decay_steps=100, decay_rate=0.96\n",
    "    )\n",
    ")\n",
    "'''\n",
    "\n",
    "content_image = preprocess_image(CONTENT_IMAGE_PATH)\n",
    "style_image = preprocess_image(STYLE_IMAGE_PATH)\n",
    "generated_image = tf.Variable(preprocess_image(CONTENT_IMAGE_PATH))\n",
    "\n",
    "iterations = 4000\n",
    "for i in range(1, iterations + 1):\n",
    "    loss, grads = compute_loss_and_grads(style_image, generated_image, content_image)\n",
    "    \n",
    "    optimizer.apply_gradients([(grads, generated_image)])\n",
    "    if i % 100 == 0:\n",
    "        print(\"Iteration %d: loss=%.2f\" % (i, loss))\n",
    "        img = deprocess_image(generated_image.numpy())\n",
    "        fname = GENERATED_IMAGE_PATH + \"_at_iteration_%d.png\" % i\n",
    "        keras.preprocessing.image.save_img(fname, img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "temporal-jones",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(GENERATED_IMAGE_PATH+ \"_at_iteration_%d.png\" % 4000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "powered-creature",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "small-standing",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
